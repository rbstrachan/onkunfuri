# -*- coding: utf-8 -*-
"""onkunfuri_abbrev.ipynb

Automatically generated by Colab.

"""

!pip -q install requests

import os, gzip
import requests

def download(url, out_path):
    r = requests.get(url, stream=True, timeout=60)
    r.raise_for_status()
    with open(out_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=1024*1024):
            if chunk:
                f.write(chunk)
    print("downloaded", out_path, f"({os.path.getsize(out_path):,} bytes)")

# KANJIDIC2 (EDRDG)
KANJIDIC_URL = "http://www.edrdg.org/kanjidic/kanjidic2.xml.gz"
download(KANJIDIC_URL, "kanjidic2.xml.gz")

# JMdictFurigana (latest release via GitHub API)
API_URL = "https://api.github.com/repos/Doublevil/JmdictFurigana/releases/latest"
release = requests.get(API_URL, timeout=60).json()

asset_url = None
for a in release.get("assets", []):
    if a.get("name") == "JmdictFurigana.txt":
        asset_url = a.get("browser_download_url")
        break

if not asset_url:
    raise RuntimeError("Could not find JmdictFurigana.txt in latest release assets.")

download(asset_url, "JmdictFurigana.txt")

!pip -q install lxml pandas jaconv

import gzip
import re
from collections import Counter

import pandas as pd
import jaconv
from lxml import etree


KANJIDIC_PATH = "kanjidic2.xml.gz"
JMDICTFURI_PATH = "JmdictFurigana.txt"

TAG_COL = "変換フラグ"
VERSION_NAME = "abbrev"
OUT_CSV = f"onkunfuri_{VERSION_NAME}.csv"

KANJI_RE = re.compile(r"[\u3400-\u4DBF\u4E00-\u9FFF\uF900-\uFAFF々〆ヵヶ]")
KANA_CHAR_RE = re.compile(r"[ぁ-ゖァ-ヺー]")
BRACKET_RE = re.compile(
    r"(?P<seg>[\u3400-\u4DBF\u4E00-\u9FFF\uF900-\uFAFF々〆ヵヶ]+)"
    r"(?:\x5B|［)(?P<r>[^]\］]+)(?:\x5D|］)"
)
REAL_KANJI_RE = re.compile(r"[\u3400-\u4DBF\u4E00-\u9FFF\uF900-\uFAFF]")

def load_kanjidic2(path_gz: str):
    d = {}
    with gzip.open(path_gz, "rb") as f:
        tree = etree.parse(f)

    for ch in tree.xpath("//character"):
        literal = ch.findtext("literal")
        if not literal:
            continue

        on = set()
        kun = set()

        for r in ch.xpath(".//reading_meaning/rmgroup/reading"):
            rtype = r.get("r_type")
            txt = (r.text or "").strip()
            if not txt:
                continue
            if rtype == "ja_on":
                on.add(jaconv.kata2hira(txt))
            elif rtype == "ja_kun":
                kun.add(txt)

        d[literal] = {"on": on, "kun": kun}

    return d


def is_kana_char(ch: str) -> bool:
    return bool(KANA_CHAR_RE.fullmatch(ch))


def load_kanji_overrides(path="additional_kanji_readings.csv"):
    overrides = {}
    try:
        odf = pd.read_csv(path).fillna("")
    except FileNotFoundError:
        return overrides

    for _, row in odf.iterrows():
        kanji = str(row.get("kanji", "")).strip()
        reading = jaconv.kata2hira(str(row.get("reading", "")).strip())
        kind = str(row.get("kind", "")).strip().lower()
        if kanji and reading and kind in ("on", "kun"):
            overrides[(kanji, reading)] = kind
    return overrides


def load_word_overrides(path="manual_onkunyomi.csv"):
    overrides = {}
    try:
        odf = pd.read_csv(path).fillna("")
    except FileNotFoundError:
        return overrides

    for _, row in odf.iterrows():
        headword = str(row.get("headword", "")).strip()
        kanji = str(row.get("kanji", "")).strip()
        reading = jaconv.kata2hira(str(row.get("reading", "")).strip())
        kind = str(row.get("kind", "")).strip().lower()

        occ_raw = str(row.get("occurrence", "")).strip()
        occ = int(occ_raw) if occ_raw else None

        if headword and kanji and reading and kind in ("on", "kun"):
            overrides[(headword, kanji, reading, occ)] = kind
    return overrides


def parse_mapping(map_str: str):
    segments = []
    map_str = (map_str or "").strip()
    if not map_str:
        return segments

    for part in map_str.split(";"):
        part = part.strip()
        if not part:
            continue

        idx, rd = part.split(":", 1)
        idx = idx.strip()
        rd = rd.strip()

        if "-" in idx:
            a, b = idx.split("-", 1)
            start, end = int(a), int(b)
        else:
            start = end = int(idx)

        segments.append((start, end, rd))

    return segments


def build_bracketed(headword: str, mapping_str: str):
    chars = list(headword)
    n = len(chars)

    segments = parse_mapping(mapping_str)
    start_map = {s: (e, r) for s, e, r in segments}

    tokens = []
    flags = []

    if any(e > s for s, e, _ in segments):
        flags.append("irregular")

    i = 0
    while i < n:
        if i in start_map:
            end, rd = start_map[i]
            seg = "".join(chars[i:end + 1])
            token = f"{seg}[{rd}]"
            i = end + 1

            while i < n and i not in start_map and is_kana_char(chars[i]):
                token += chars[i]
                i += 1

            tokens.append(token)
        else:
            ch = chars[i]
            if tokens and is_kana_char(ch):
                tokens[-1] += ch
            else:
                tokens.append(ch)
            i += 1

    return " ".join(tokens), " ".join(sorted(set(flags)))


def normalize_kun_string(k: str) -> str:
    return (k or "").strip().strip("-")


def normalize_on_string(s: str) -> str:
    return (s or "").strip().strip("-")


GODAN_RENYOUKEI = {
    "う": "い",
    "く": "き",
    "ぐ": "ぎ",
    "す": "し",
    "つ": "ち",
    "ぬ": "に",
    "ぶ": "び",
    "む": "み",
    "る": "り",
}

GODAN_MIZENKEI = {
    "う": "わ",
    "く": "か",
    "ぐ": "が",
    "す": "さ",
    "つ": "た",
    "ぬ": "な",
    "ぶ": "ば",
    "む": "ま",
    "る": "ら",
}

RENDAKU_MAP = {
    "か": "が", "き": "ぎ", "く": "ぐ", "け": "げ", "こ": "ご",
    "さ": "ざ", "し": "じ", "す": "ず", "せ": "ぜ", "そ": "ぞ",
    "た": "だ", "ち": "ぢ", "つ": "づ", "て": "で", "と": "ど",
    "は": "ば", "ひ": "び", "ふ": "ぶ", "へ": "べ", "ほ": "ぼ",
}

HANDAKU_MAP = {"は": "ぱ", "ひ": "ぴ", "ふ": "ぷ", "へ": "ぺ", "ほ": "ぽ"}


def kun_variants(kun: str):
    k = normalize_kun_string(kun)
    if "." not in k:
        return {k}

    stem, okuri = k.split(".", 1)
    out = {stem, stem + okuri}

    if len(okuri) >= 2:
        out.add(stem + okuri[:-1])

    if len(okuri) == 1:
        if okuri in GODAN_RENYOUKEI:
            out.add(stem + GODAN_RENYOUKEI[okuri])
        if okuri in GODAN_MIZENKEI:
            out.add(stem + GODAN_MIZENKEI[okuri])

    return out


def on_variants(on_hira: str):
    vs = {on_hira}
    if on_hira and on_hira[-1] in ("く", "き", "つ", "ち"):
        vs.add(on_hira[:-1] + "っ")
    return vs


def rendaku_variants(reading: str):
    reading = (reading or "").strip()
    if not reading:
        return set()

    first = reading[0]
    rest = reading[1:]
    out = set()
    if first in RENDAKU_MAP:
        out.add(RENDAKU_MAP[first] + rest)
    if first in HANDAKU_MAP:
        out.add(HANDAKU_MAP[first] + rest)
    return out


def classify_on_kun_single_kanji(kanji: str, reading_hira: str, kd):
    info = kd.get(kanji)
    if not info:
        return "unknown"

    f = (reading_hira or "").strip()

    on_set = set()
    for o in info["on"]:
        o = normalize_on_string(o)
        for v in on_variants(o):
            on_set.add(v)
            on_set |= rendaku_variants(v)

    kun_set = set()
    for k in info["kun"]:
        for v in kun_variants(k):
            v = v.strip()
            kun_set.add(v)
            kun_set |= rendaku_variants(v)

    is_on = f in on_set
    is_kun = f in kun_set

    if is_on and is_kun:
        return "ambiguous"
    if is_on:
        return "on"
    if is_kun:
        return "kun"
    return "unknown"

def kun_forms_for_prefix_check(kanji: str, kd):
    info = kd.get(kanji)
    if not info:
        return set()

    out = set()
    for k in info["kun"]:
        k = normalize_kun_string(k)
        if "." in k:
            continue

        for v in kun_variants(k):
            v = (v or "").replace(".", "").strip()
            if v:
                out.add(v)
    return out

def is_voiced_only_on_match(kanji: str, reading_hira: str, kd) -> bool:
    info = kd.get(kanji)
    if not info:
        return False

    f = (reading_hira or "").strip()
    base_on = set()
    voiced_on = set()

    for o in info["on"]:
        o = normalize_on_string(o)
        for v in on_variants(o):
            base_on.add(v)
            voiced_on |= rendaku_variants(v)

    return (f not in base_on) and (f in voiced_on)

def is_suspect_abbrev_on_token(kanji: str, reading_hira: str, kd) -> bool:
    r = (reading_hira or "").strip()
    if not r:
        return False

    if not is_voiced_only_on_match(kanji, r, kd):
        return False

    kun_forms = kun_forms_for_prefix_check(kanji, kd)
    return any(k.startswith(r) and len(k) > len(r) for k in kun_forms)

def convert_onkun_in_text(text: str, kd, headword: str = None, word_overrides=None, kanji_overrides=None):
    word_overrides = word_overrides or {}
    kanji_overrides = kanji_overrides or {}

    flags = []
    out_parts = []
    last_end = 0
    last_base_kanji = None
    occ_counter = {}

    text = str(text)

    for m in BRACKET_RE.finditer(text):
        out_parts.append(text[last_end:m.start()])

        seg = m.group("seg")
        r_raw = m.group("r")
        r_norm = jaconv.kata2hira(r_raw)
        original = m.group(0)

        if re.search(r"[ァ-ヺ]", r_raw):
            flags.append("source_katakana")

        if len(seg) == 1 and KANJI_RE.fullmatch(seg):
            base_kanji = seg

            if seg == "々":
                if last_base_kanji is None:
                    flags.append("unknown")
                    out_parts.append(original)
                    last_end = m.end()
                    continue
                base_kanji = last_base_kanji

            key_occ = (seg, r_norm)
            occ_counter[key_occ] = occ_counter.get(key_occ, 0) + 1
            occ = occ_counter[key_occ]

            forced_kind = None

            if headword is not None:
                forced_kind = word_overrides.get((headword, seg, r_norm, occ))
                if forced_kind is None:
                    forced_kind = word_overrides.get((headword, seg, r_norm, None))
                if forced_kind in ("on", "kun"):
                    flags.append("manual_onkunyomi")

            if forced_kind is None:
                forced_kind = kanji_overrides.get((base_kanji, r_norm))
                if forced_kind in ("on", "kun"):
                    flags.append("additional_kanji_reading")

            if forced_kind in ("on", "kun"):
                kind = forced_kind
            else:
                kind = classify_on_kun_single_kanji(base_kanji, r_norm, kd)
                if kind == "on" and is_suspect_abbrev_on_token(base_kanji, r_norm, kd):
                    flags.append("suspect_abbrev")
                    flags.append("abbrev_on_to_kun")
                    kind = 'kun'

            if kind == "on":
                flags.append("on")
                out_parts.append(f"{seg}[{jaconv.hira2kata(r_norm)}]")
            elif kind == "kun":
                flags.append("kun")
                out_parts.append(f"{seg}[{r_norm}]")
            else:
                flags.append(kind)
                out_parts.append(f"{seg}[{r_raw}]")

            if seg != "々":
                last_base_kanji = seg
        else:
            flags.append("irregular")
            out_parts.append(original)

        last_end = m.end()

    out_parts.append(text[last_end:])
    return "".join(out_parts), " ".join(sorted(set(flags)))

def classify_on_kun_single_kanji_audit(kanji: str, reading_hira: str, kd):
    kind = classify_on_kun_single_kanji(kanji, reading_hira, kd)
    if kind != "on":
        return kind, False

    info = kd.get(kanji)
    if not info:
        return kind, False

    f = (reading_hira or "").strip()

    base_on_set = set()
    voiced_on_set = set()

    for o in info["on"]:
        o = normalize_on_string(o)
        base_on_set |= on_variants(o)
        voiced_on_set |= rendaku_variants(o)

    on_voiced = (f not in base_on_set) and (f in voiced_on_set)
    return kind, on_voiced

def process_jmdictfurigana(txt_path: str, kd, tag_col: str, word_overrides=None, kanji_overrides=None, limit=None):
    rows = []
    with open(txt_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if limit is not None and i >= limit:
                break

            line = line.strip()
            if not line or line.startswith("#"):
                continue

            parts = line.split("|")
            if len(parts) != 3:
                continue

            headword, yomi, mapping = parts
            furi1, flags1 = build_bracketed(headword, mapping)
            furi2, flags2 = convert_onkun_in_text(
                furi1,
                kd,
                headword=headword,
                word_overrides=word_overrides,
                kanji_overrides=kanji_overrides,
            )

            flags = " ".join(sorted(set((flags1 + " " + flags2).split()))).strip()
            rows.append({"語彙": headword, "読み方": yomi, "振り仮名": furi2, tag_col: flags})

    return pd.DataFrame(rows)


def load_exclude_list(path="filter.txt"):
    exclude = set()
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                exclude.add(line)
    except FileNotFoundError:
        pass
    return exclude


def apply_filters(df, headword_col="語彙", exclude_path="filter.txt"):
    before = len(df)

    exclude = load_exclude_list(exclude_path)
    if exclude:
        df = df[~df[headword_col].astype(str).isin(exclude)]
    after_exclude = len(df)

    has_real_kanji = df[headword_col].astype(str).str.contains(REAL_KANJI_RE, na=False)
    df = df[has_real_kanji]
    after_kanji = len(df)

    return df, {
        "before": before,
        "excluded_by_filter_txt": before - after_exclude,
        "removed_no_real_kanji": after_exclude - after_kanji,
        "after": after_kanji,
    }


def summary_table(df, tag_col: str, version_name: str):
    total = len(df)

    def has_tag(tag: str):
        return df[tag_col].fillna("").str.contains(rf"(?:^|\s){re.escape(tag)}(?:\s|$)", regex=True)

    def pct(x):
        return 0.0 if total == 0 else (x / total * 100.0)

    unknown_mask = has_tag("unknown")
    ambiguous_mask = has_tag("ambiguous")
    irregular_mask = has_tag("irregular")

    unknown_count = int(unknown_mask.sum())
    ambiguous_count = int(ambiguous_mask.sum())
    irregular_count = int(irregular_mask.sum())

    clean_mask = ~(unknown_mask | ambiguous_mask)
    clean_count = int(clean_mask.sum())

    overlap_count = int((unknown_mask & ambiguous_mask).sum())
    either_count = int((unknown_mask | ambiguous_mask).sum())

    print("| version | count | tag:unknown | tag:ambiguous | tag:irregular | -tag:unknown -tag:ambiguous | tag:unknown or tag:ambiguous | overlap |")
    print("| ------- | ----- | ----------- | ------------- | ------------- | --------------------------- | ---------------------------- | ------- |")
    print(
        f"| {version_name} | "
        f"{total:,} | "
        f"{unknown_count:,}<br>({pct(unknown_count):.2f}%) | "
        f"{ambiguous_count:,}<br>({pct(ambiguous_count):.2f}%) | "
        f"{irregular_count:,}<br>({pct(irregular_count):.2f}%) | "
        f"{clean_count:,}<br>({pct(clean_count):.2f}%) | "
        f"{either_count:,}<br>({pct(either_count):.2f}%) | "
        f"{overlap_count:,}<br>({pct(overlap_count):.2f}%) |"
    )

    all_tags = Counter()
    for cell in df[tag_col].fillna("").astype(str):
        for t in cell.split():
            all_tags[t] += 1

    print()
    for tag, cnt in all_tags.most_common(100):
        print(f"{tag:25s} {cnt:10,} ({pct(cnt):5.2f}%)")
    print()


kd = load_kanjidic2(KANJIDIC_PATH)
KANJI_OVERRIDES = load_kanji_overrides()
WORD_OVERRIDES = load_word_overrides()

print("KANJIDIC2 loaded:", len(kd), "/ 13108\n")
print("loaded additional kanji readings:", len(KANJI_OVERRIDES))
print("loaded manual onkunyomi:", len(WORD_OVERRIDES))

test_out, test_flags = convert_onkun_in_text("水[すい] 曜[よう] 日[び]", kd)
assert test_out.startswith("水[スイ] 曜[ヨウ]")
print("\nself test successful\n")

df = process_jmdictfurigana(
    JMDICTFURI_PATH,
    kd,
    tag_col=TAG_COL,
    word_overrides=WORD_OVERRIDES,
    kanji_overrides=KANJI_OVERRIDES,
)

df, filter_stats = apply_filters(df, headword_col="語彙", exclude_path="filter.txt")
print(filter_stats, "\n")

summary_table(df, TAG_COL, VERSION_NAME)

df.to_csv(OUT_CSV, index=False)
print("output file:", OUT_CSV)

from collections import Counter, defaultdict

def rank_unknown_pairs(
    df,
    kd,
    top_n=50,
    examples_per=5,
    word_overrides=None,
    kanji_overrides=None,
):
    """
    Ranks (kanji, reading) pairs that are still 'unknown' AFTER applying:
      1) per-word overrides (optionally occurrence-specific)
      2) global kanji reading overrides
      3) normal KANJIDIC2-based classification

    reading is normalized to hiragana for matching.
    """
    word_overrides = word_overrides or {}
    kanji_overrides = kanji_overrides or {}

    pair_counts = Counter()
    pair_examples = defaultdict(list)

    for _, row in df.iterrows():
        headword = str(row["語彙"])
        text = str(row["振り仮名"])

        last_base_kanji = None
        occ_counter = {}  # (seg, reading_hira) -> count within this headword

        for m in BRACKET_RE.finditer(text):
            seg = m.group("seg")
            r_raw = m.group("r")
            r_norm = jaconv.kata2hira(r_raw).strip()

            # only per-kanji classification for single kanji / 々
            if not (len(seg) == 1 and KANJI_RE.fullmatch(seg)):
                continue

            base = seg
            if seg == "々":
                if last_base_kanji is None:
                    # can't resolve noma -> treat as unknown of 々 itself (optional)
                    base = "々"
                else:
                    base = last_base_kanji
            else:
                last_base_kanji = seg

            # occurrence counting (1-based)
            occ_key = (seg, r_norm)
            occ_counter[occ_key] = occ_counter.get(occ_key, 0) + 1
            occ = occ_counter[occ_key]

            # --- Apply overrides (same precedence as convert_onkun_in_text) ---
            forced_kind = None

            # 1) word overrides (exact occurrence, then any occurrence)
            forced_kind = word_overrides.get((headword, seg, r_norm, occ))
            if forced_kind is None:
                forced_kind = word_overrides.get((headword, seg, r_norm, None))

            # 2) kanji overrides (global) - note: use base kanji for 々 too
            if forced_kind is None:
                forced_kind = kanji_overrides.get((base, r_norm))

            # If overridden, it's not a "failure" anymore
            if forced_kind in ("on", "kun"):
                continue

            # 3) normal classification
            kind = classify_on_kun_single_kanji(base, r_norm, kd)

            if kind == "unknown":
                pair_counts[(base, r_norm)] += 1
                if len(pair_examples[(base, r_norm)]) < examples_per:
                    pair_examples[(base, r_norm)].append(headword)

    print(f"Top {top_n} unknown (kanji, reading) pairs (after overrides):")
    for (k, r), c in pair_counts.most_common(top_n):
        ex = ", ".join(pair_examples[(k, r)])
        print(f"{c:6d}  {k}[{r}]　→　{ex}")

# Usage:
rank_unknown_pairs(
    df, kd,
    top_n=50,
    examples_per=25,
    word_overrides=WORD_OVERRIDES,
    kanji_overrides=KANJI_OVERRIDES,
)